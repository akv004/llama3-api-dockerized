version: '3.8'

services:
  # Service for the Redis database
  redis:
    image: redis:7-alpine
    container_name: llama3-redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis_data:/data
    restart: unless-stopped

  # Service for your Llama3 FastAPI application
  llama3-api:
    build: .
    container_name: llama3-api
    depends_on:
      - redis
    ports:
      - "8002:8002"
    volumes:
      # --- Mounts your local directories into the container ---
      - /home/amit/projects/.cache/huggingface:/home/appuser/.cache/huggingface
      - /home/amit/projects/logs:/home/appuser/app/logs
      - /home/amit/projects/my_rag_data:/home/appuser/app/faiss_index # Changed from _no_rag for clarity
    environment:
      # --- Replicates your script's environment variables ---
      # CRITICAL: Use the service name 'redis', not 'localhost'
      - REDIS_URL=redis://redis:6379/0
      
      # Paths below are the paths INSIDE the container
      - BASE_DIR=/home/appuser/app
      - HF_HOME=/home/appuser/.cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/home/appuser/.cache/huggingface/hub
      - FAISS_DIR=/home/appuser/app/faiss_index

      # Other variables from your script
      - EMBED_MODEL=BAAI/bge-small-en-v1.5
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      
      # This passes your token from your host shell into the container
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
    restart: unless-stopped
    deploy:
      # --- This section grants the container access to your NVIDIA GPU ---
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]